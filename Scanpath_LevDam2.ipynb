{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd84aa5a",
   "metadata": {},
   "source": [
    "# Scan Path Analysis pt2\n",
    "#### @tutor: Ms SHARAFI Zohreh\n",
    "#### @student: Mr SHAW Oscar\n",
    "\n",
    "### The purpose of this notebook is to analyze and compare problem solving strategies between different participants thanks to the similarity between their scanpaths\n",
    "\n",
    "#### How do we proceed: \n",
    "There, we start the analysis by focusing only on the sequence of entities seen by the participant and measure the distance between two sequences thanks to the Damerau-Levenshtein algorithms, the interest of this method is that it takes into acount the transposition of two adjacent characters. Instead of comparing paths between differents participants, there we'll compare paths for differents code for the same participant to see if a participant always follow the same problem solving strategy.\n",
    "\n",
    "A guess would be that someone with a strong background should know best problem solving methods and therefore will always follow the same strategy. Thus its sequences must be close, that is to say a high normalized score.\n",
    "\n",
    "Then we study the differences between sequences for a same task for two differents participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c37b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# !pip install pandas\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214128c",
   "metadata": {},
   "source": [
    "## 1- Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77684a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the folder which contains the files\n",
    "FOLDER_PATH = \"C:\\\\Users........\"\n",
    "DATA_FOLDER = os.path.join(FOLDER_PATH, 'data_files')\n",
    "\n",
    "# Dictionary establishing the correspondence between a file and its sequence of entities\n",
    "# i.e data{key = file_id, value = [seq. of entities]} where entities are in [‘Comment’, ‘Bug_Report’, ‘Member_Variable’, \n",
    "# ‘Method_Body’, ‘Method_Signature’]\n",
    "# ex: file_id = P_103 & value = ['Comment','Bug_Report','None','Bug_Report']\n",
    "data = {}\n",
    "\n",
    "\n",
    "# For each csv file, we use the dataframe to populate the dictionary with the data \n",
    "for filename in os.listdir(DATA_FOLDER):\n",
    "    df = pd.read_csv(os.path.join(DATA_FOLDER, filename),delimiter=',')\n",
    "    data[filename.split('_')[0]] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b47bae",
   "metadata": {},
   "source": [
    "## 2- Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3c7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataframe from NONE feature values\n",
    "def clean(df: object, features: [str]) -> object:\n",
    "#     input:  df, features  -> object (dataframe), array of string containing the feature we want to filter \n",
    "#     output: df -> object (dataframe)   \n",
    "    for i in features:\n",
    "        indexNames = df[ df[i] == 'NONE' ].index\n",
    "        df.drop(indexNames , inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c647a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to only keep the features we want to use in our study\n",
    "def selection(df: object, features: [str]) -> object:\n",
    "#     input:  df, features  -> object (dataframe), array of string containing the feature we want to keep \n",
    "#     output: df -> object (dataframe)\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfeddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to convert entity name to letter for latter treatments \n",
    "def convert(df):\n",
    "#     input: df -> object (dataframe) with sequence of entities in plain text\n",
    "#     output: df -> object (dataframe) with sequence of letter to embodies entities\n",
    "    return df.applymap(lambda x: 'A' if x.lower() == 'method_body' else ('B' if x.lower() == 'comment' else ('C' if x.lower() == 'member_variable' else ('D' if x.lower() == 'bug_report' else ('E' if x.lower() == 'method_signature' else pd.NA)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7552e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the sequence of letter into a string for latter treatments\n",
    "def SequenceToString(df):\n",
    "#     input: df -> object (dataframe)\n",
    "#     output: string -> string which embodies the sequence of entities with letters\n",
    "    return ''.join( ''.join(map(str, x)) for x in df.entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8632f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the DF regarding the phase number\n",
    "def divide(df,i):\n",
    "#     input:  df, i  -> object (dataframe) large dataframe with all phase number, int number of phase_number \n",
    "#     output: df -> object (dataframe) array of df depending on the phase number \n",
    "    inp  = [df.loc[df['phase_number'] == i+1, 'entity'].to_frame() for i in range(i)]\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db9dd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to clean NA values\n",
    "def cleanNA(df):\n",
    "#     input:  df  -> object (dataframe) with NA  \n",
    "#     output: df -> object (dataframe) without NA and reindexed \n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55ededeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P102': ['BDBCCBDDBCBBBABEBAAAAAAAAAAAAAAAABBBAAABAAABBABBBABBABAAABABBBBAABBBABBBACCBABAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBAAAAAABABAABBBAAABAAEAEAAAAAAEABBAAAAAACAAAAAABBBBBCBAABBBCCBCBCCCCAAAAAABAABBBBBAAABAAAAAAAAAAAAAACAAAAAABAAAABAABABBAAAAABAABBABAAAAAAAAABAAAAAABAAAAABAAAAAAAAAABAAAAAAABBAAAAAAAAAAABAABAABAAAAAAABABABBABBABABBAAABAAAAABABAAAAAAAAAAAAAAAAAAAAAABABAAAAAAAAAAAAAAAAAAAABABBBAAAAABAAAAAAAABBABBBBABAABAAAABAACAAABB',\n",
       "  'AAAAAAAAAABEBBEEAAAABEAAEAAAAAAAAAAABAABABAAAAAABAAAAAAAAAABAAAAAABABAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBAABBCCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABABBBBBAAAAAAAAABAAAAAAABCAABAAAAAAAAAAEAAAAAAAAAAAAAAABAAAAABBAAAAAAAAAAAAAAAAAAAABAAAAAAABBABBEAACAAAABABBAABAAABCBBBBCBCCBBAAAAABAAAAABBABBBAAAAABABBBABACBAAAAABBAABAAAAAAAAAAABAABAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAABAAAAAAAAAABBBAAAAAAEABAAAAAAAABBAAABABBBAAAABEAAAAABBEAABBBAEABBABAAAEAAAAAAAAAAAAAAABAAAABBBAAAAAABBAABAAAABABABAAABAAAAABAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBABBBAABBBACEBEEABBAAAAABAABBBBBBAAAAABAAAAAAAAAAAABAABAAAAAAAAA',\n",
       "  'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAABBBBABBEEEEABBBEBBAAE'],\n",
       " 'P103': ['ABAEABABAAAAAAAAAAAAAABAABBAAABAAAAAAAABAAABABBBAAABABBBABAAACCCAAAABAAAA',\n",
       "  'AAAABABAAAAAAAAAAAAAAAAAEABAAAAAAAAAAAAAAAAAAABBBBCCCCCBBBCBCCAAAABBBABBEBBBAAAABBAAAACCCBCCBCCCCCCCCCCBAAAABAAAABBAAAABABBAAABAAAAAAAAAAAAAAAAAAAAAAEEEBBAABAABBAABAAABBABBBBAABAAABBAAABABABBAABBBCBBBAAAAABBBAAAAABBBABAAAAAAAAAAAAAABAAAAAAAAAABBBAAAAAAAACCBAAABAAABBBBBCCCCCCCBBBBBBBBBBBABBAABEAAAAABAAAAAAAAAABBCCCCCAACAACAACCAAABABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCAABBBBBAAAABBBAABAABAABAAAABBBAEAAABABAAAACCCCAAAABABAABBBAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAABBAAAABAAAAAAAAAAAAAAAAAAAABAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA',\n",
       "  'AAAAAAAAAAA'],\n",
       " 'P105': ['ABAAABABAAAAABEAABBBBABAEBAAAEAEAAABAAABAAABABAABBABBBABAAABAAAABBBAAAAABBBBAABBAAAAAAAABABABAAAAAAAAAAABBBBBBBBAAABAAAABBBBBAAAAEAAAAAAAAABBBAAAAAAAAAAAAAABBBAAAAABBAAAAAAAAAAAAAAAAAAAAABABAAAAAAABAAAABABAAABBAAAAAAAAAAAAAAABBABBAAABAAAAAAAAAAABAABBABBBABBAAAEEEBBAABABBBAAAAAAABAAAAABAAAAAABABBBBEBBBBBABAAAEBBBABBABBBBAABAAAAAAEBAAABBBABBAAEABCAAAAAAAAAAAAAABAABCAAAAAAAAAAAAAAAAAAAAAAABAAEAAAABAAAAAAAAAAAA',\n",
       "  'AAAAAAAAAAAAABABAEABBAAAAAAAABAAAAAAAACAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBAAABBAAAABAAAABAAAAAAAAAAAAAAAABBABBBAAAEBAAAAAAAAAAAABAABBAABAAABAAABBBBABABABAAAAAAAAAAAABAAAAAAAAAABAAAAAAAAAAAABBBBBBBBBBAABABBBBBBBAAAAEBAAABABAAAAAAABAABBBBBBAABAABAABBAAAAAABBBBAAABAABBBAABBBABAAAAABAAAAAAAAAAABAAAAAABABABBBBAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAABABBABABAAAAAAAAAAAAAAAAAAAAAEABEBABCCCCCCCCCCCCCCBCCCCCCBBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBBABAAAAAAAAAAABAAABAAAAEAAABBBAAAAAAABBBBAABABAAAAAAAAAAAAAAABAAAAAAAAAABABACAABAAAAAAAAAAAABCCCAAAAAAAAAAAAAA'],\n",
       " 'P106': ['',\n",
       "  'AAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAABAAAAAAAAABBAAABAABBABAAABBBAABABBAABBAAAABABAAABBAAAAAAAABABBAABBBAAAAAAAACCCAACAABAAABAAAAAAAAAAAAAAAAAAAAAAAAAABBAAABAAAAAAAAAAABAAAABAAAAAAAAAAAAAAAAAAAAABAABAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBBAAAAAAAAAABAAAAAAAAAAAAAAAAABAAAA',\n",
       "  'AAAAABAAAAAAAAAAAAAAAAAAABAAAAAAAAABAADABBAAAAAAAAAACAABAAAABAAAAABBABBAAAAABAAABA'],\n",
       " 'P107': ['BBCBBBBBCBDDDDBAAAAABABAAAABBBEBABBAAAAABBABAAAABBAAAA',\n",
       "  'AAAAAAABAABAAAAABAAABBBBAAAAAAAABAAAAAABAAAAAAAAAAABBAAAAAABAAAEBBABAACAAABAAAAAAAAABAAAAABAAABAAAAAAAAAAAAAAAAAAAAAAAAAAABBAAAAAAEEBAAAAAAAAABBBAAAAAAAAAAAAAAAAAAAAAAAAABAAABAAAAAABBBEBBBBBAABBAABBBBBAABBBBBBBABBBBBBBABBBBABABAAAAABAAABABABBAABBAAABABAABBABAAAAAAAAAABBAAAAAAAAABABEAAAAAAAABBBAAAAAAAAABAABABBABBBBBBBAAAAAABAAABBBBBAAAAABBAAAAAAAABAAAAABAABBBBAAABBAAAAAABAAAAAAAABAAAAAAAAAAAAAAABBAAAABBBBBABBBBBBBBBBBBBBBEAEAEAAAAEAAAAAAAABABAAAAAABBAAAAAAAAAAAAAEBBAAAABBBAABAAAAAAAAAABBAAABBAABAABBBBAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAABBBBBAAAAAABBBBBBABBBAAAAAACCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAABBAAABBEEBBBAAABBBABAAAAAAAAAAAAAAAAAAAAABABAAABBBAAAAAAAAAABAAAABAAAAAAABAAAAAAAAAAAAAAAAABABBAAAAAAAAABABAAAAAAAABBAAEAAABAAAAABBBABAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAABAAAABABAAAAAAAAAAAAAAAAAAAAAAAAABABABABBBBABAAAAABBBBBAAAAAAAAAAABBABBABAAAABBAABAABAAAABBABAAAAAAAAABBBBBBBBABBBAABAAAAAAAAAAAAAAAABBBAAEBBBBAAAAAABAAAAAAAAAAAABBACAAAAAAAAAAABAAAAAAAABBAAAAAAAAAAA',\n",
       "  'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAABBABABAAABBABBBBBABBBBBBBAAABBBBBABBAAABABBAABBBBBBAAABBBBBABBBBBBBAAAABBBBBBABBBABBAAABBAAABBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBAABBBBBBBBBBBBABBBBBBBBBBBBBABBBBBBBBBBBBBABBBBBBBBBABBBBAAAABAEEBBAABBBAAABBBBAAAAAABBBABAAAAAAAABBBBAABBBBBBBBBBBABBBBBBBBBAABBBBBBBBBBBABBBBAABBBABAABBAAAAAAAAABAAAAABBAAAAAAAAAAAABAAAAAAAAAAAAAABBBBAAAEBAAAAAAAAAABBAABAAAAAAAABABBBAAAABBBBAEEACCCEAAAAAAAAAAADBBBABBAAABBBBAABAAAAEBAAAAAAAABBAAAAABBBBABBBBABAAAAAEABAAAABAAAAAABBAAAAABAAAAAAAAAAAABABBAAAAAAAAAAAAAAAAAABAAAAAAAAAAAABAABABAAAABBAABBBBAABAABAABACAAAABAAAAAAAAAAAAAAABAAAABBABBAAABAAAABAAAAABAABBABBEAABAEAAAAAAAAABAAAEBAAAABBAAAAA']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over each dataframes of data and clean data in each, then convert the data.\n",
    "# There we use another dictionnary for more reusability of past data\n",
    "def pipeline(data):\n",
    "#      input: data, features  -> dictionnary{key = file_id, value = object (dataframe)}, array of string containing the feature we want to keep\n",
    "#      output: new_data -> dictionnary{key = file_id, value = [sequency]}   \n",
    "    new_data = {}\n",
    "    # divide dataframe regarding the phase_number\n",
    "    for i in data:\n",
    "        divided_df = divide(data[i],max(data[i]['phase_number']))\n",
    "        res = []\n",
    "        for df in divided_df:\n",
    "            df = convert(df)\n",
    "            df = cleanNA(df)\n",
    "            # df to String\n",
    "            df = SequenceToString(df)       \n",
    "            res.append(df)\n",
    "        new_data[i] = res\n",
    "    return new_data\n",
    "\n",
    "new_data = pipeline(data)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb8d79",
   "metadata": {},
   "source": [
    "### Damerau-Levenshtein algorithm - Iterative version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77620c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def damerauLevenshtein(s1: str, s2: str) -> float:\n",
    "#      input: s1, s2  -> str, str: the two strings that are compared \n",
    "#      output: normalized_score -> float: normalized score between the strings\n",
    "    score_matrix = np.zeros((len(s1)+1,len(s2)+1),int) #we define a matrix which will contains the scores\n",
    "    score_matrix[:,0] = np.linspace(0, len(s1),len(s1)+1) #fulfill the first column of the matrix with 1,2,3,4,...\n",
    "    score_matrix[0,:] = np.linspace(0, len(s2),len(s2)+1) #fulfill the first row of the matrix with 1,2,3....\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            if s1[i] == s2[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            score_matrix[i+1,j+1] = min(score_matrix[i,j]+cost, # score by alignement\n",
    "                                    score_matrix[i,j+1]+1, # score by deletion\n",
    "                                    score_matrix[i+1,j]+1) # score by insertion\n",
    "            \n",
    "            if i > 0 and j > 0 and s1[i] == s2[j-1] and s1[i-1] == s2[j]:\n",
    "                score_matrix[i+1,j+1] = min (score_matrix[i+1,j+1],  score_matrix[i-1,j-1] + 1) # score by transposition\n",
    "    distance = float(score_matrix[-1,-1])\n",
    "    normalized_score = 1.0-distance/max(len(s1),len(s2))\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576190e",
   "metadata": {},
   "source": [
    "### Levenshtein algorithm - Iterative version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b31deb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following, the iterative version of Levenshetin algorithm (using matrix) which return the normalized distance between two strings\n",
    "def iterLevenshtein(s1: str, s2: str) -> float: \n",
    "#      input: s1, s2  -> str, str: the two strings that are compared \n",
    "#      output: normalized_score -> float: normalized score between the strings\n",
    "    score_matrix = np.zeros((len(s1)+1,len(s2)+1),int) #we define a matrix which will contains the scores\n",
    "    score_matrix[:,0] = np.linspace(0, len(s1),len(s1)+1) #fulfill the first column of the matrix with 1,2,3,4,...\n",
    "    score_matrix[0,:] = np.linspace(0, len(s2),len(s2)+1) #fulfill the first row of the matrix with 1,2,3....\n",
    "\n",
    "    for i in range(1,len(s1)+1):\n",
    "        for j in range(1,len(s2)+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            score_matrix[i,j] = min(score_matrix[i-1,j-1]+cost, # score by alignement\n",
    "                                    score_matrix[i-1,j]+1, # score by deletion\n",
    "                                    score_matrix[i,j-1]+1) # score by insertion\n",
    "    distance = float(score_matrix[-1,-1])\n",
    "    normalized_score = 1.0-distance/max(len(s1),len(s2))\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25ac00",
   "metadata": {},
   "source": [
    "## 3 - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae91af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_dam = {}\n",
    "intra_lev = {}\n",
    "#foreach doc\n",
    "for x in new_data.items():\n",
    "    tab_dam = []\n",
    "    tab_lev = []\n",
    "    for y in range(len(x[1])-1):\n",
    "        for z in range(y+1,len(x[1])):\n",
    "            # compare only non-empty strings of a same df\n",
    "            if x[1][y] and x[1][z]:\n",
    "                tab_dam.append((y+1,z+1,round(damerauLevenshtein(x[1][y],x[1][z]),5)))\n",
    "                tab_lev.append((y+1,z+1,round(iterLevenshtein(x[1][y],x[1][z]),5)))\n",
    "\n",
    "    intra_dam[x[0]] = tab_dam\n",
    "    intra_lev[x[0]] = tab_lev    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c42f1cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P102': [(1, 2, 0.62171), (1, 3, 0.27232), (2, 3, 0.21217)],\n",
       " 'P103': [(1, 2, 0.13153), (1, 3, 0.15068), (2, 3, 0.01982)],\n",
       " 'P105': [(1, 2, 0.57047)],\n",
       " 'P106': [(2, 3, 0.29242)],\n",
       " 'P107': [(1, 2, 0.0444), (1, 3, 0.07091), (2, 3, 0.50622)]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intra_dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92c515a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P102': [(1, 2, 0.62171), (1, 3, 0.27232), (2, 3, 0.21217)],\n",
       " 'P103': [(1, 2, 0.13153), (1, 3, 0.15068), (2, 3, 0.01982)],\n",
       " 'P105': [(1, 2, 0.57047)],\n",
       " 'P106': [(2, 3, 0.29242)],\n",
       " 'P107': [(1, 2, 0.0444), (1, 3, 0.07091), (2, 3, 0.50622)]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intra_lev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa42b4",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Score has to be interpreted as (taskA, taskB, score(taskA,taskB))\n",
    "\n",
    "Here we compare the strategies used between the differents programs for a same participant, the higher the score between two phases, the more the sequences of entities are similar and therefore the more the participant tends to follow the same resolution strategy in his differents tasks\n",
    "\n",
    "##### As in the previous analysis, we can see that P102 and P105 are the two with the highest score, for task 1 & 2 which may be adapted to this kind of strategy. It would seem that they are the most coherent in their strategy and that their strategies are close, a first hypothesis would be that they are the two most experienced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fea28",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    " Previously we compare the complete sequence between candidates and the partials sequences for a same candidates, so let's compare the partials sequences for different candidates to check if the firsts results are always correct when we affine the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0cddf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "alreadyMade = []\n",
    "correlated_dam = {}\n",
    "correlated_lev = {}\n",
    "for x in new_data.items():\n",
    "    for y in new_data.items():\n",
    "        partialsSeq_dam = []\n",
    "        partialsSeq_lev = []\n",
    "        for i in range(len(x[1])):\n",
    "            if y not in alreadyMade and i< len(y[1]) and x[0]!=y[0] and x[1][i] and y[1][i]:\n",
    "                partialsSeq_dam.append((i+1,round(damerauLevenshtein(x[1][i],y[1][i]),5)))\n",
    "                partialsSeq_lev.append((i+1,round(iterLevenshtein(x[1][i],y[1][i]),5)))\n",
    "        if partialsSeq_dam:\n",
    "            correlated_dam['_'.join((x[0],y[0]))] = partialsSeq_dam\n",
    "            correlated_lev['_'.join((x[0],y[0]))] = partialsSeq_lev\n",
    "    alreadyMade.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c3ec794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P102_P103': [(1, 0.15625), (2, 0.66118), (3, 0.08527)],\n",
       " 'P102_P105': [(1, 0.65179), (2, 0.68914)],\n",
       " 'P102_P106': [(2, 0.44737), (3, 0.54264)],\n",
       " 'P102_P107': [(1, 0.10938), (2, 0.49911), (3, 0.18669)],\n",
       " 'P103_P105': [(1, 0.17561), (2, 0.63758)],\n",
       " 'P103_P106': [(2, 0.48649), (3, 0.13415)],\n",
       " 'P103_P107': [(1, 0.50685), (2, 0.4325), (3, 0.01592)],\n",
       " 'P105_P106': [(2, 0.45302)],\n",
       " 'P105_P107': [(1, 0.11707), (2, 0.49023)],\n",
       " 'P106_P107': [(2, 0.24512), (3, 0.11867)]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56ddfc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P102_P103': [(1, 0.15625), (2, 0.66283), (3, 0.08527)],\n",
       " 'P102_P105': [(1, 0.65179), (2, 0.68914)],\n",
       " 'P102_P106': [(2, 0.44737), (3, 0.54264)],\n",
       " 'P102_P107': [(1, 0.10938), (2, 0.49911), (3, 0.18669)],\n",
       " 'P103_P105': [(1, 0.17561), (2, 0.63758)],\n",
       " 'P103_P106': [(2, 0.48649), (3, 0.13415)],\n",
       " 'P103_P107': [(1, 0.50685), (2, 0.4325), (3, 0.01592)],\n",
       " 'P105_P106': [(2, 0.45302)],\n",
       " 'P105_P107': [(1, 0.11707), (2, 0.49023)],\n",
       " 'P106_P107': [(2, 0.24512), (3, 0.11867)]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_dam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889df8e8",
   "metadata": {},
   "source": [
    "### Average score analysis for Levenshtein Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98b1a5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P102_P103', 0.3009),\n",
       " ('P102_P105', 0.67046),\n",
       " ('P102_P106', 0.495),\n",
       " ('P102_P107', 0.26506),\n",
       " ('P103_P105', 0.4066),\n",
       " ('P103_P106', 0.31032),\n",
       " ('P103_P107', 0.31842),\n",
       " ('P105_P106', 0.45302),\n",
       " ('P105_P107', 0.30365),\n",
       " ('P106_P107', 0.1819)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_lev = [(x[0], round(np.array([y[1] for y in x[1]]).mean(),5)) for x in correlated_lev.items()]\n",
    "avg_lev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c670c26",
   "metadata": {},
   "source": [
    "### Average score analysis for Damerau-Levenshtein Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d09429a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P102_P103', 0.30145),\n",
       " ('P102_P105', 0.67046),\n",
       " ('P102_P106', 0.495),\n",
       " ('P102_P107', 0.26506),\n",
       " ('P103_P105', 0.4066),\n",
       " ('P103_P106', 0.31032),\n",
       " ('P103_P107', 0.31842),\n",
       " ('P105_P106', 0.45302),\n",
       " ('P105_P107', 0.30365),\n",
       " ('P106_P107', 0.1819)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_dam = [(x[0], round(np.array([y[1] for y in x[1]]).mean(),5)) for x in correlated_dam.items()]\n",
    "avg_dam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b1280",
   "metadata": {},
   "source": [
    "## Results\n",
    "We can see that on both task 1 & 2, P102 and P105 have high score, so their sequences are close, it confirm the hypothesis emitted in part1, that they follow close strategy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
